<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="References MPoL introduction Machine Learning: A Probabilistic Perspective by Murphy, Chapter 10 Pattern Recognition and Machine Learning by Bishop, Chapter 8 The fourth paper in the 2019 Event Horizon Telescope Collaboration series describing the imaging principles Maximum entropy image restoration in astronomy AR&A by Narayan and Nityananda 1986 Multi-GPU maximum entropy image synthesis for radio astronomy by Cárcamo et al. 2018 Regularized Maximum Likelihood Techniques for ALMA Observations by Zawadzki, Czekala, et al."><meta name=generator content="Hugo 0.104.3"><title>Regularized Maximum Likelihood (RML) I | Czekala Group</title><link rel=canonical href=/courses/astro589/lectures/09-rml-part-i/><script async src="https://www.googletagmanager.com/gtag/js?id=G-W5586VESJK"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W5586VESJK",{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/vega@5.20.2></script>
<script src=https://cdn.jsdelivr.net/npm/vega-lite@5.1.0></script>
<script src=https://cdn.jsdelivr.net/npm/vega-embed@6.17.0></script>
<link rel=stylesheet href=/css/base.min.e9a98f863db62272e3c41bb8784b2588cd338ebba264b169f66d736c05d1b47f.css integrity="sha256-6amPhj22InLjxBu4eEsliM0zjruiZLFp9m1zbAXRtH8=" crossorigin=anonymous></head><body><nav class=u-background><div class=u-wrapper><ul class=Banner><li class="Banner-item Banner-item--title"><a class="Banner-link u-clickable" href=/>Czekala Group</a></li><li class=Banner-item><a class="Banner-link u-clickable" href=/people/>People</a></li><li class=Banner-item><a class="Banner-link u-clickable" href=/posts/>Posts</a></li><li class=Banner-item><a class="Banner-link u-clickable" href=/publications/>CV and publications</a></li><li class=Banner-item><a class="Banner-link u-clickable" href=/courses/>Courses</a></li></ul></div></nav><main><div class=u-wrapper><div class=u-padding><main><article><header><h1>Regularized Maximum Likelihood (RML) I</h1></header><h2 id=references>References</h2><ul><li><a href=https://mpol-dev.github.io/MPoL/rml_intro.html>MPoL introduction</a></li><li><a href=https://catalog.libraries.psu.edu/catalog/19499523>Machine Learning: A Probabilistic Perspective</a> by Murphy, Chapter 10</li><li><a href=https://catalog.libraries.psu.edu/catalog/3405468>Pattern Recognition and Machine Learning</a> by Bishop, Chapter 8</li><li>The fourth paper in the 2019 <a href=https://ui.adsabs.harvard.edu/abs/2019ApJ...875L...4E/abstract>Event Horizon Telescope Collaboration series</a> describing the imaging principles</li><li><a href=https://ui.adsabs.harvard.edu/abs/1986ARA%26A..24..127N/abstract>Maximum entropy image restoration in astronomy</a> AR&A by Narayan and Nityananda 1986</li><li><a href=https://ui.adsabs.harvard.edu/abs/2018A%26C....22...16C/abstract>Multi-GPU maximum entropy image synthesis for radio astronomy</a> by Cárcamo et al. 2018</li><li><a href=https://ui.adsabs.harvard.edu/abs/2022arXiv220911813Z/abstract>Regularized Maximum Likelihood Techniques for ALMA Observations</a> by Zawadzki, Czekala, et al.</li></ul><h2 id=last-time>Last time</h2><ul><li>Discussed \(u,v\) coverage and sampling (weights)</li><li>Introduced the &ldquo;dirty image&rdquo; as the inverse Fourier transform of the visibility samples</li><li>Introduced the CLEAN image deconvolution procedure</li></ul><h2 id=this-time>This time</h2><ul><li>Review parametric vs. non-parametric models</li><li>Introduce Regularized Maximum Likelihood (RML) imaging</li></ul><h2 id=parametric-vs-non-parametric-models>Parametric vs. Non-Parametric Models</h2><p>Recall our discussion on Bayesian inference and what it means to forward-model a dataset and how to calculate a likelihood function.</p><p>Say we have a model that we are fitting to some data. In &ldquo;Machine Learning: A Probabilistic Perspective,&rdquo; Murphy defines a parametric model as one that has a fixed number of parameters, and a non-parametric one as one where the number of parameters grows with the size of the data.</p><p>Something like the line we discussed in our Bayesian Modeling lecture is a parametric model. It has two parameters, a slope and an intercept. If we were observing a source and we wanted to fit a 2D Gaussian to the visibility function, that visibility model would also be a parametric model. Its parameters would be the width of the Gaussian, the position of the source, and the amplitude of the source.</p><p>TODO: draw example of Gaussian function and label parameters</p><p>If you&rsquo;ve ever fit a spline to a bunch of data, then you&rsquo;ve used a non-parametric model. A Gaussian process would also a non-parametric model. In these models there in fact are parameters (like the number or type of splines/GP kernels), but these are usually nuisances to the problem. You wouldn&rsquo;t necessarily fit a spline model to determine the exact number of spline position parameters, but you <em>are</em> interested in the approximation to \(f(x)\) that the spline has enabled you.</p><p>TODO: draw points and a spline or GP drawn through them</p><p>I would consider a CLEAN model to be a type of non-parametric model too. Through the CLEANing process, you create a model of the source emission from a collection of \(\delta\) functions. Each of these \(\delta\) functions technically has parameters, but those are mostly nuisance parameters in pursuit of their aggregate representation of the model. In general, non-parametric models have the ability to be more expressive than parametric models, but sometimes at the expense of interpretability.</p><h2 id=rml-images-as-non-parametric-models>RML images as non-parametric models</h2><p>Now, let me introduce a set of techniques that have been grouped under the banner &ldquo;Regularized Maximum Likelihood Imaging&rdquo; or RML Imaging for short. With RML imaging, we&rsquo;re trying to come up with a model that will fit the dataset. But rather than using a parametric model like a protoplanetary disk structure model or a series of Gaussian rings, we&rsquo;re using a non-parametric model of <em>the image itself</em>. This could be as simple as parameterizing the image using the intensity values of the pixels themselves, i.e.,</p><p>$$
\boldsymbol{\theta} = {I_1, I_2, \ldots, I_{N^2} }
$$</p><p>assuming we have an \(N \times N\) image.</p><p>A flexible image model for RML imaging is mostly analogous to using a spline or Gaussian process to fit a series of \(\boldsymbol{X} = {x_1, x_2, \ldots, x_N}\) and \(\boldsymbol{Y} = {y_1, y_2, \ldots, y_N}\) points&mdash;the model will nearly always have enough flexibility to capture the structure that exists in the dataset. The most straightforward formulation of a non-parametric image model is the pixel basis set, but we could also use more sophisticated basis sets like a set of wavelet coefficients, or even more exotic basis sets constructed from trained neural networks. These may have some serious advantages when it comes to the &ldquo;regularizing&rdquo; part of &ldquo;regularized maximum likelihood&rdquo; imaging. But first, let&rsquo;s talk about the &ldquo;maximum likelihood&rdquo; part.</p><p>Given some image parameterization (e.g., a pixel basis set of \(N \times N\) pixels, with each pixel <code>cell_size</code> in width), we would like to find the maximum likelihood image \(\boldsymbol{\theta}_\mathrm{MLE}\). Fortunately, because the Fourier transform is a linear operation, we can analytically calculate the maximum solution (the same way we might find the best-fit slope and intercept for the line example). This maximum likelihood solution is called (in the radio astronomy world) the dirty image, and its associated point spread function is called the dirty beam.</p><p>In the construction of the dirty image, all unsampled spatial frequencies are set to zero power. This means that the dirty image will only contain spatial frequencies about which we have at least some data. This assumption, however, rarely translates into good image fidelity, especially if there are many unsampled spatial frequencies which carry significant power. It&rsquo;s also important to recognize that dirty image is only <em>one</em> out of a set of <em>many</em> images that could maximize the likelihood function. From the perspective of the likelihood calculation, we could modify the unsampled spatial frequencies of the dirty image to whatever power we might like, and, because they are <em>unsampled</em>, the value of the likelihood calculation won&rsquo;t change, i.e., it will still remain maximal.</p><p>When synthesis imaging is described as an &ldquo;ill-posed inverse problem,&rdquo; this is what is meant. There is a (potentially infinite) range of images that could <em>exactly</em> fit the dataset, and without additional information we have no way of discriminating which is best. As you might suspect, this is now where the &ldquo;regularization&rdquo; part of &ldquo;regularized maximum likelihood&rdquo; imaging comes in.</p><h2 id=regularization>Regularization</h2><p>There are a number of different ways to talk about regularization. If one wants to be Bayesian about it, one would talk about specifying <em>priors</em>, i.e., we introduce terms like \(p(\boldsymbol{\theta})\) such that we might calculate the maximum a posteriori (MAP) image \(\boldsymbol{\theta}_\mathrm{MAP}\) using the posterior probability distribution</p><p>$$
p(\boldsymbol{\theta} |\, \boldsymbol{V}) \propto \mathcal{L}(\boldsymbol{V} |\, \boldsymbol{\theta}) \, p(\boldsymbol{\theta}).
$$</p><p>For computational reasons related to numerical over/underflow, we would most likely use the logarithm of the posterior probability distribution</p><p>$$
\ln p(\boldsymbol{\theta} |\, \boldsymbol{V}) \propto \ln \mathcal{L}(\boldsymbol{V} |\, \boldsymbol{\theta}) + \ln p(\boldsymbol{\theta}).
$$</p><p>One could accomplish the same goal without necessarily invoking the Bayesian language by simply talking about which parameters \(\boldsymbol{\theta}\) optimize some objective function.</p><p>We&rsquo;ll adopt the perspective that we have some objective &ldquo;cost&rdquo; function that we&rsquo;d like to <em>minimize</em> to obtain the optimal parameters \(\hat{\boldsymbol{\theta}}\). The machine learning community calls this a &ldquo;loss&rdquo; function \(L(\boldsymbol{\theta})\), and so we&rsquo;ll borrow that terminology here. For an unregularized fit, an acceptable loss function is just the negative log likelihood (&ldquo;nll&rdquo;) term,</p><p>$$
L(\boldsymbol{\theta}) = L_\mathrm{nll}(\boldsymbol{\theta}) = - \ln \mathcal{L}(\boldsymbol{V}|\,\boldsymbol{\theta}) = \frac{1}{2} \chi^2(\boldsymbol{V}|\,\boldsymbol{\theta})
$$</p><p>If we&rsquo;re only interested in \(\hat{\boldsymbol{\theta}}\), it doesn&rsquo;t matter whether we include the 1/2 prefactor in front of \(\chi^2\), the loss function will still have the same optimum. However, when it comes time to add additional terms to the loss function, these prefactors matter in controlling the relative strength of each term.</p><p>When phrased in the terminology of function optimization, additional terms can be described as regularization penalties. To be specific, let&rsquo;s add a term that regularizes the sparsity of an image.</p><p>$$
L_\mathrm{sparsity}(\boldsymbol{\theta}) = \sum_i |I_i|
$$</p><p>In short, the L1 norm promotes sparse solutions (solutions where many pixel values are zero). The combination of these two terms leads to a new loss function</p><p>$$
L(\boldsymbol{\theta}) = L_\mathrm{nll}(\boldsymbol{\theta}) + \lambda_\mathrm{sparsity} L_\mathrm{sparsity}(\boldsymbol{\theta})
$$</p><p>Where we control the relative &ldquo;strength&rdquo; of the regularization via the scalar prefactor \(\lambda_\mathrm{sparsity}\). If \(\lambda_\mathrm{sparsity} = 0\), no sparsity regularization is applied. Non-zero values of \(\lambda_\mathrm{sparsity}\) will add in regularization that penalizes non-sparse \(\boldsymbol{\theta}\) values. How strong this penalization is depends on the strength relative to the other terms in the loss calculation.</p><p>We can equivalently specify this using Bayesian terminology, such that</p><p>$$
p(\boldsymbol{\theta} |\,\boldsymbol{V}) = \mathcal{L}(\boldsymbol{V}|,\boldsymbol{\theta}) \, p(\boldsymbol{\theta})
$$</p><p>where</p><p>$$
p(\boldsymbol{\theta}) = C \exp \left (-\lambda_\mathrm{sparsity} \sum_i | I_i| \right)
$$</p><p>and \(C\) is a normalization factor. When working with the logarithm of the posterior, this constant term is irrelevant.</p><h2 id=the-mpol-package-for-regularized-maximum-likelihood-imaging>The MPoL package for Regularized Maximum Likelihood imaging</h2><p><em>Million Points of Light</em> or &ldquo;MPoL&rdquo; is a Python package that is used to perform regularized maximum likelihood imaging. By that we mean that the package provides the building blocks to create flexible image models and optimize them to fit interferometric datasets. The package is developed completely in the open on <a href=https://github.com/MPoL-dev/MPoL>Github</a></p><p>We strive to</p><ul><li>create an open, welcoming, and supportive community for new users and contributors (see our <code>code of conduct &lt;https://github.com/MPoL-dev/MPoL/blob/main/CODE_OF_CONDUCT.md></code><strong>and <code>developer documentation &lt;developer-documentation.html></code></strong>)</li><li>support well-tested (|Tests badge|) and stable releases (i.e., <code>pip install mpol</code>) that run on all currently-supported Python versions, on Linux, MacOS, and Windows</li><li>maintain up-to-date <code>API documentation &lt;api.html></code>__</li><li>cultivate tutorials covering real-world applications</li></ul><p>We also recommend checking out several other excellent packages for RML imaging:</p><ul><li><a href=https://github.com/astrosmili/smili>SMILI</a></li><li><a href=https://github.com/achael/eht-imaging>eht-imaging</a></li><li><a href=https://github.com/miguelcarcamov/gpuvmem>GPUVMEM</a></li></ul><p>There are a few things about MPoL that we believe make it an appealing platform for RML modeling.</p><ul><li><p><strong>Built on PyTorch</strong>: Many of MPoL&rsquo;s exciting features stem from the fact that it is built on top of a rich computational library that supports autodifferentiation and construction of complex neural networks. Autodifferentiation libraries like <a href=https://github.com/aesara-devs/aesara>Theano/Aesara</a>, <a href=https://www.tensorflow.org/>Tensorflow</a>, <a href=https://pytorch.org/>PyTorch</a>, and <a href=https://jax.readthedocs.io/>JAX</a> have revolutionized the way we compute and optimize functions. For now, PyTorch is the library that best satisfies our needs, but we&rsquo;re keeping a close eye on the Python autodifferentiation ecosystem should a more suitable framework arrive. If you are familiar with scientific computing with Python but haven&rsquo;t yet tried any of these frameworks, don&rsquo;t worry, the syntax is easy to pick up and quite similar to working with numpy arrays.</p></li><li><p><strong>Autodifferentiation</strong>: PyTorch gives MPoL the capacity to autodifferentiate through a model. The <em>gradient</em> of the objective function is exceptionally useful for finding the &ldquo;downhill&rdquo; direction in a large parameter space (such as the set of image pixels). Traditionally, these gradients would have needed to been calculated analytically (by hand) or via finite-difference methods which can be noisy in high dimensions. By leveraging the autodifferentiation capabilities, this allows us to rapidly formulate and implement complex prior distributions which would otherwise be difficult to differentiate by hand.</p></li><li><p><strong>Optimization</strong>: PyTorch provides a full-featured suite of research-grade <a href=https://pytorch.org/docs/stable/optim.html>optimizers</a> designed to train deep neural networks. These same optimizers can be employed to quickly find the optimum RML image.</p></li><li><p><strong>GPU acceleration</strong>: PyTorch wraps CUDA libraries, making it seamless to take advantage of (multi-)GPU acceleration to optimize images. No need to use a single line of CUDA.</p></li><li><p><strong>Model composability</strong>: Rather than being a monolithic program for single-click RML imaging, MPoL strives to be a flexible, composable, RML imaging <em>library</em> that provides primitives that can be used to easily solve your particular imaging challenge. One way we do this is by mimicking the PyTorch ecosystem and writing the RML imaging workflow using <a href=https://pytorch.org/tutorials/beginner/nn_tutorial.html>PyTorch modules</a>. This makes it easy to mix and match modules to construct arbitrarily complex imaging workflows. We&rsquo;re working on tutorials that describe these ideas in depth, but one example would be the ability to use a single latent space image model to simultaneously fit single dish and interferometric data.</p></li><li><p><strong>A bridge to the machine learning/neural network community</strong>: MPoL will happily calculate RML images for you using &ldquo;traditional&rdquo; image priors, lest you are the kind of person that turns your nose up at the words &ldquo;machine learning&rdquo; or &ldquo;neural network.&rdquo; However, if you are the kind of person that sees opportunity in these tools, because MPoL is built on PyTorch, it is straightforward to take advantage of them for RML imaging. For example, if one were to train a variational autoencoder on protoplanetary disk emission morphologies, the latent space + decoder architecture could be easily plugged in to MPoL and serve as an imaging basis set.</p></li></ul></article></main></div></div></main><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>