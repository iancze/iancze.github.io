<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Video recording References for today The Fourier Transform and its Applications by R. Bracewell Interferometry and Synthesis in Radio Astronomy by Thompson, Moran, and Swenson, particularly Appendix 2.1 Fourier Analysis and Imaging by R. Bracewell Wikipedia on Nyqist-Shannon sampling theorem Review of last time Defined Fourier transform, inverse $$ F(s) = \int_{-\infty}^{\infty} f(x) e^{-i 2 \pi x s}\,\mathrm{d}x $$
Note that because \(x\) and \(s\) appear in the argument of the exponential, their product must be dimensionless."><meta name=generator content="Hugo 0.104.1"><title>The Fourier Transform II | Czekala Group</title><link rel=canonical href=/courses/astro589/lectures/03-fourier-transform-ii/><script async src="https://www.googletagmanager.com/gtag/js?id=G-W5586VESJK"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-W5586VESJK",{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/vega@5.20.2></script>
<script src=https://cdn.jsdelivr.net/npm/vega-lite@5.1.0></script>
<script src=https://cdn.jsdelivr.net/npm/vega-embed@6.17.0></script>
<link rel=stylesheet href=/css/base.min.e9a98f863db62272e3c41bb8784b2588cd338ebba264b169f66d736c05d1b47f.css integrity="sha256-6amPhj22InLjxBu4eEsliM0zjruiZLFp9m1zbAXRtH8=" crossorigin=anonymous></head><body><nav class=u-background><div class=u-wrapper><ul class=Banner><li class="Banner-item Banner-item--title"><a class="Banner-link u-clickable" href=/>Czekala Group</a></li><li class=Banner-item><a class="Banner-link u-clickable" href=/people/>People</a></li><li class=Banner-item><a class="Banner-link u-clickable" href=/posts/>Posts</a></li><li class=Banner-item><a class="Banner-link u-clickable" href=/publications/>CV and publications</a></li><li class=Banner-item><a class="Banner-link u-clickable" href=/courses/>Courses</a></li></ul></div></nav><main><div class=u-wrapper><div class=u-padding><main><article><header><h1>The Fourier Transform II</h1></header><ul><li><a href=https://psu.mediaspace.kaltura.com/media/Astro+589A+Lecture+3/1_607jh3p1>Video recording</a></li></ul><h2 id=references-for-today>References for today</h2><ul><li><a href=https://catalog.libraries.psu.edu/catalog/2010095>The Fourier Transform and its Applications</a> by R. Bracewell</li><li><a href=https://catalog.libraries.psu.edu/catalog/20789467>Interferometry and Synthesis in Radio Astronomy</a> by Thompson, Moran, and Swenson, particularly Appendix 2.1</li><li><a href=https://catalog.libraries.psu.edu/catalog/34517505>Fourier Analysis and Imaging</a> by R. Bracewell</li><li><a href=https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem>Wikipedia on Nyqist-Shannon sampling theorem</a></li></ul><h2 id=review-of-last-time>Review <em>of last time</em></h2><ul><li><p>Defined Fourier transform, inverse
$$
F(s) = \int_{-\infty}^{\infty} f(x) e^{-i 2 \pi x s}\,\mathrm{d}x
$$</p></li><li><p>Note that because \(x\) and \(s\) appear in the argument of the exponential, their product must be dimensionless. This means that they will also have inverse units, e.g., &ldquo;seconds&rdquo; and &ldquo;cycles per second.&rdquo;</p></li><li><p>Introduced convolution, impluse symbol, and theorems</p></li></ul><h2 id=where-are-we-headed-today>Where are we headed <em>today</em>?</h2><ul><li>Finish up Fourier transform theorems</li><li>Nyquist sampling theorem</li><li>Discrete Fourier Transform</li></ul><h2 id=continuing-from-last-time-fourier-transform-theorems>Continuing from last time: <em>Fourier transform theorems</em></h2><h3 id=convolutionmultiplication>Convolution/multiplication</h3><p>The convolution of two functions corresponds to the multiplication of their Fourier transforms.</p><p>If</p><p>$$
f(x) \leftrightharpoons F(s)
$$</p><p>and</p><p>$$
g(x) \leftrightharpoons G(s)
$$</p><p>then</p><p>$$
f(x) * g(x) \leftrightharpoons F(s)G(s).
$$</p><p>This is an <em>extremely</em> useful theorem. At least in my career, this, and concepts related to sampling, have been the ones I have used the most often. You may have already used this theorem (numerically) if you&rsquo;ve ever carried out a convolution operation using <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.fftconvolve.html>scipy.signal.fftconvolve</a> in Python, which can be dramatically faster than directly implementing the convolution, at least for certain array sizes.</p><h3 id=rayleighs-theorem-parsevals-theorem-for-fourier-series>Rayleigh&rsquo;s theorem (Parseval&rsquo;s theorem for Fourier Series)</h3><p>The amount of energy in a system is the same whether you calculate it in the time domain or in the frequency domain.</p><p>The integral of the mod-squared of a function is equal to the integral of the mod-squared of its spectrum</p><p>$$
\int_{-\infty}^\infty |f(x)|^2\,\mathrm{d}x = \int_{-\infty}^\infty |F(s)|^2\,\mathrm{d}s.
$$</p><h3 id=autocorrelation-theorem>Autocorrelation theorem</h3><p>The autocorrelation of a function is</p><p>$$
f(x) * f(x) = \int_{-\infty}^\infty f^{*}(u) f(u + x)\,\mathrm{d}u
$$</p><p>and it has the Fourier transform
$$
f(x) * f(x) \leftrightharpoons |F(s)|^2.
$$</p><p>Thus, the power spectrum is the Fourier transform of the autocorrelation function. It can also be computed directly by taking the &ldquo;mod-squared&rdquo; of \(F(s)\),</p><p>$$
|F|^2 = F F^*.
$$</p><p>If you&rsquo;ve ever worked with (stationary) Gaussian processes (e.g., squared-exponential, Matern, etc&mldr;), you might recognize this relationship between the autocorrelation (the kernel function) and the power spectrum of the Gaussian process.</p><h3 id=the-derivative-theorem>The derivative theorem</h3><p>If</p><p>$$
f(x) \leftrightharpoons F(s)
$$</p><p>then</p><p>$$
f^\prime(x) \leftrightharpoons i 2 \pi s F(s).
$$</p><h3 id=using-the-transform-pairs>Using the transform pairs</h3><p>Now that we have a few basic transform pairs, and some of the transform theorems, you can mix and match these to build up a library of new transform pairs. You will explore this in the problem set.</p><h3 id=definite-integral>Definite integral</h3><p>The zero-valued frequency of a Fourier transform is equal to the definite integral of a function over all space</p><p>$$
\int_{-\infty}^\infty f(x)\,\mathrm{d}x = F(0).
$$</p><p>I.e., to compute the area under the curve, you can just read off the zero-frequency value of the Fourier transform.</p><h3 id=first-moment>First moment</h3><p>The first moment of \(f(x)\) about the origin is</p><p>$$
\int_{-\infty}^\infty x f(x)\,\mathrm{d}x
$$</p><p>Using the derivative theorem (not shown), you can determine that the first moment \(\bar{f}\) is</p><p>$$
\bar{f} = \int_{-\infty}^\infty x f(x)\,\mathrm{d}x = \frac{F^\prime(0)}{-2 \pi i}
$$
i.e., the slope of the Fourier transform evaluated at \(s = 0\), times \(-1/2 \pi i\).</p><h3 id=second-moment>Second moment</h3><p>Second moment (moment of inertia) is given by</p><p>$$
\int_{-\infty}^\infty x^2 f(x)\,\mathrm{d}x = -\frac{1}{4 \pi^2} F^{\prime\prime}(0).
$$</p><figure><img src=second-moment.png alt="Credit: Bracewell Fig 8.4"><figcaption><p>Credit: Bracewell Fig 8.4</p></figcaption></figure><p>Similar arguments extend to variance \(\sigma^2\) as we had mentioned with regards to the uncertainty principle.</p><h2 id=smoothness-and-compactness>Smoothness and compactness</h2><p>In general, the smoother a function is in the time domain, the more compact its Fourier transform will be in the frequency domain.</p><p>Smoother functions will have a larger number of continuous derivatives. Something like the Gaussian envelope \(\exp(-\pi x^2)\) is &ldquo;as smooth as possible&rdquo; and therefore its Fourier transform (also a Gaussian envelope) is as compact as possible.</p><h2 id=filters-and-transfer-functions>Filters and transfer functions</h2><h3 id=time-domain>Time domain</h3><p>We can say that we have some (electrical) waveform
$$
V_1(t) = A \cos (2 \pi f t)
$$ which is a single-valued function of time. You can think of this as a voltage time-series or another physical quantity. By definition, the waveform is real.</p><p>Let&rsquo;s put on our electrical/acoustical/mechanical engineering hats for a moment and consider that a <em>filter</em> is a physical system with an input an and output, e.g., something that is transmitting vibrations or oscillations, like our waveform.</p><figure><img src=filter.png alt="How a filter changes the amplitude and phase of an waveform. Credit: Ian Czekala"><figcaption><p>How a filter changes the amplitude and phase of an waveform. Credit: Ian Czekala</p></figcaption></figure><p>If we feed our waveform into a <em>linear</em> filter, we get output
$$
V_2(t) = B \cos (2 \pi f t + \phi).
$$</p><p>The output is still a waveform, but its amplitude and its phase have changed. These changes are likely frequency dependent, too.</p><p>We can specify the filter by a frequency-dependent quantity \(T(f)\) called the <em>transfer function</em>. It is a complex-valued function (having both an amplitude and a phase) and is given by
$$
T(f) = \frac{B}{A}e^{i \phi}.
$$</p><p>Interesting, perhaps, but maybe not immediately obviously useful. Let&rsquo;s introduce the <em>spectrum</em> and then circle back to the transfer function.</p><h3 id=obtaining-v_2-using-the-frequency-domain>Obtaining \(V_2\) using the frequency domain</h3><p>The <em>spectrum</em> of the waveform is the Fourier transform of \(V(t)\), which we&rsquo;ll call \(S(f)\) in this section. We&rsquo;ve broken slightly from our \(f \leftrightharpoons F\) notation, but \(V \leftrightharpoons S\) is a classic in the signal processing and electrical engineering fields, so we&rsquo;ll at least build familiarity with it in this example.</p><p>The &ldquo;spectrum&rdquo; here is just the Fourier transform quantity, it can definitely be complex-valued.</p><p>The &ldquo;spectra&rdquo; that we typically talk about in astrophysics are measurements of the electromagnetic spectrum&mdash;you&rsquo;ve probably never come across as one that&rsquo;s complex-valued, right? What&rsquo;s going on here?</p><p>Consider the <em>units</em> of a flux measurement \(F_\nu\) of the electromagnetic spectrum. In lecture 1, we covered that the cgs units of flux are
$$
\mathrm{ergs}\;\mathrm{s}^{-1}\;\mathrm{cm}^{-2}\;\mathrm{Hz}^{-1}.
$$</p><p>The clue is in the \(\mathrm{ergs}\;\mathrm{s}^{-1}\) part, which we could also write in terms of &ldquo;watts&rdquo; if we wanted to be strictly S.I. about it. When we are measuring the electromagnetic spectrum, we are actually measuring the <strong>power</strong> spectral density, \(|F(\nu)|^2\). The absolute squared means the quantity \(|F(\nu)|^2\) is real-valued, and is the reason why you never hear about measurements of the electromagnetic spectrum containing imaginary values!</p><p>In this course, at least, we&rsquo;ll try to be explicit about which spectrum we&rsquo;re referring to. When we actually mean power spectrum, we&rsquo;ll try to call it as such. Otherwise, &ldquo;spectrum&rdquo; will refer to a quantity like \(S\).</p><p>Now, let&rsquo;s revisit our filter example, where we had input and output \(V_1(t)\) and \(V_2(t)\) respectively. From our discussion, we also have</p><p>$$
V_1 \leftrightharpoons S_1
$$
and
$$
V_2 \leftrightharpoons S_2.
$$</p><p>The transfer function concept is <em>especially</em> useful when we think about the <em>spectrum</em> of the waveforms, because we have
$$
S_2(f) = T(f) S_1(f).
$$</p><p>I.e., the spectrum of the output waveform is simply the spectrum of the input waveform <em>multiplied</em> by the transfer function.</p><p>Once you have \(S_2\), then you can get \(V_2(t)\) from
$$
V_2 \leftrightharpoons S_2.
$$</p><p>Two examples are low-pass and high-pass filters.</p><figure><a href=https://en.wikipedia.org/wiki/Filter_%28signal_processing><img src=filter-pass.png alt="Examples of different filter transfer functions \(T(f)\). Credit: Wikipedia/SpinningSpark"></a><figcaption><p>Examples of different filter transfer functions \(T(f)\). Credit: Wikipedia/SpinningSpark</p></figcaption></figure><h3 id=obtaining-v_2-using-the-time-domain>Obtaining \(V_2\) using the time domain</h3><p>Now let&rsquo;s think of digital signal processing, where you wanted to practically apply a filter to some waveform to produce a new waveform. As we just outlined, you could acquire \(V_1(t)\), Fourier transform it to access its spectrum \(S_1(f)\), multiply by the transfer function \(T_(f)\), and then do the inverse Fourier transform to get \(V_2(t)\). Is there a way to do this <em>directly</em> in the time domain? What if you don&rsquo;t have the complete waveform all at once?</p><p>The answer is provided by the convolution theorem for Fourier transforms. Since the transfer function is applied via a multiplication in the Fourier domain, we could equivalently carry out the same operation by a convolution in the time domain.</p><p>The convolutional kernel would be
$$
I(t) \leftrightharpoons T(f)
$$
and we&rsquo;d have
$$
V_2(t) = I(t) * V_1(t).
$$</p><p>To summarize,<figure><img src=square.png alt="Credit Bracewell, Chapter 9."><figcaption><p>Credit Bracewell, Chapter 9.</p></figcaption></figure></p><h3 id=determining-it>Determining \(I(t)\)</h3><p>If you had some system already in place, and you wanted to determine \(I(t)\) experimentally, what is one way you could do it? What waveform could you send the system?</p><p>One simple option would be to send
$$
V_1(t) = \delta(t),
$$
then
$$
V_2(t) = I(t) * \delta(t) = I(t).
$$</p><h2 id=nyquist-shannon-sampling>Nyquist-Shannon sampling</h2><ul><li><a href="https://www.youtube.com/watch?v=FcXZ28BX-xE&ab_channel=SteveBrunton">Youtube/SteveBrunton</a> on The Sampling Theorem</li></ul><p>Thus far we have been talking about continuous functions. As astrophysicists, though, we&rsquo;re frequently dealing with discrete data points, which are presumed to be <em>samples</em> of some unknown function. Maybe you&rsquo;re the one designing the experiment to capture these data points, or maybe you&rsquo;ve just been handed some dataset.</p><figure><img src=lines.png alt="Say you are given a set of (noisless) samples that look like this. What do you think the function should look like in between the points? Credit: Ian Czekala"><figcaption><p>Say you are given a set of (noisless) samples that look like this. What do you think the function should look like in between the points? Credit: Ian Czekala</p></figcaption></figure><p>Concisely put, the <em>sampling theorem</em> states that under a certain condition, a function can be <em>completely</em> reconstructed from a set of discrete samples&mdash;without information loss. I.e., the set of discrete samples is <em>fully equivalent</em> to having access to the full set of function values. Today, this sampling theorem is known as the Nyquist-Shannon sampling theorem, the Whittaker–Nyquist–Shannon theorem, or simply &ldquo;the sampling theorem.&rdquo;</p><p>If we were to try to use these data points to actually reconstruct a function, then what sort of constraint would we need to impose on the function? We&rsquo;d want to place some constraint on its spectrum, i.e., that there are no higher frequency components oscillating around faster than our sampling points.</p><p>Before we dive into the derivation of the sampling theorem, let&rsquo;s first take another look at what can go wrong when you undersample a time series.</p><h3 id=aliasing>Aliasing</h3><figure><img src=sine-wave.png alt="If the true signal is given by the solid line, but we undersample it, then the sine-wave we naively reconstruct from the samples would have the wrong frequency. Here we would say that the higher frequency signal has been aliased into the lower frequency range. Credit: Wikipedia Pluke"><figcaption><p>If the true signal is given by the solid line, but we <em>undersample</em> it, then the sine-wave we naively reconstruct from the samples would have the wrong frequency. Here we would say that the higher frequency signal has been aliased into the lower frequency range. Credit: Wikipedia Pluke</p></figcaption></figure><p>This is also called the Stroboscopic effect. There are some nice examples online:</p><ul><li><a href="https://www.youtube.com/watch?v=dNVtMmLlnoE&ab_channel=SmarterEveryDay">Youtube/SmarterEveryDay</a>.</li><li><a href="https://www.youtube.com/watch?v=smDpCsVVgPA&ab_channel=Edyourself">Stationary helicopter</a>.</li><li>Exoplanet transits! <a href=https://ui.adsabs.harvard.edu/abs/2010ApJ...722..937D/abstract>Dawson and Fabrycky 2010</a> find a shorter period for the exoplanet 55 Cnc e, previously confounded by the timing of the RV observations.</li></ul><h3 id=derivation-of-sampling-theorem>Derivation of sampling theorem</h3><p>Now, let&rsquo;s use our understanding of Fourier transforms and the sampling/replicating function to develop a precise formulation of the sampling theorem.</p><p>Recall that the &ldquo;shah&rdquo; function is an infinite series of delta functions spaced a unit dimension apart, and that it is its own Fourier transform
$$
\mathrm{shah}(x) \leftrightharpoons \mathrm{shah}(s).
$$</p><p>Via the similarity theorem, if the delta functions of the shah get closer in the \(x\) domain, then they spread out in the Fourier domain, and vice-versa.</p><p>We can adjust the spacing of the samples by dilating or shrinking the shah function by some factor. Here, we&rsquo;ll write this as the sampling interval \(\Delta x = \tau\) or the sampling frequency \(1/\tau\).</p><p>According to the similarity theorem, adjusting the sampling frequency in the \(x\) domain has the following effect in the frequency domain
$$
\mathrm{shah}(x/\tau) \leftrightharpoons \tau \mathrm{shah}(\tau s).
$$</p><p>For example, if \(\tau = 0.2\), then we have</p><figure><img src=panel-b.png alt="The shah function is its own Fourier transform. Via the similarity theorem, if we compress the shah function in the time domain (left), we expand it in the Fourier domain (right). Credit: Bracewell, Fig 10.3"><figcaption><p>The shah function is its own Fourier transform. Via the similarity theorem, if we compress the shah function in the time domain (left), we expand it in the Fourier domain (right). Credit: Bracewell, Fig 10.3</p></figcaption></figure><p>Now let&rsquo;s consider a function and its Fourier transform</p><figure><img src=function.png alt="A generic function (left) and its Fourier transform (right). We say that this function is &amp;lsquo;band-limited&amp;rsquo; because its Fourier transform is 0 for all frequencies below some cutoff frequency \(|s| &amp;lt; s_c\). Credit: Bracewell Fig 10.2"><figcaption><p>A generic function (left) and its Fourier transform (right). We say that this function is &lsquo;band-limited&rsquo; because its Fourier transform is 0 for all frequencies below some <em>cutoff frequency</em> \(|s| &lt; s_c\). Credit: Bracewell Fig 10.2</p></figcaption></figure><p>As before, we will use multiplication by the shah function to represent sampling of the function.</p><figure><img src=sampled-function.png alt="Left: the sampled version of \(f\), which has the Fourier transform on the right. So long as the sampling frequency exceeds twice the cutoff frequency, the Fourier transform &amp;lsquo;islands&amp;rsquo; do not overlap (top two rows). Credit: Bracewell Fig 10.3"><figcaption><p>Left: the sampled version of \(f\), which has the Fourier transform on the right. So long as the sampling frequency exceeds twice the cutoff frequency, the Fourier transform &lsquo;islands&rsquo; do not overlap (top two rows). Credit: Bracewell Fig 10.3</p></figcaption></figure><p>The non-overlappingness of the &lsquo;islands&rsquo; is the key to properly sampling a function, and we&rsquo;ll see why in a moment when we talk about reconstruction. But first, let&rsquo;s make a quantitative statement of the sampling theorem (Bracewell):</p><p>If \(s_c\) is the cutoff frequency defining the band-limited nature of the signal, then so long as the function is sampled at equal intervals not exceeding \(\Delta x = 1/(2 s_c)\) then the function is properly sampled, i.e.
$$
\frac{1}{\tau} \geq 2 s_c.
$$</p><h3 id=restoration-of-signal-kernels>Restoration of signal kernels</h3><p>Now let&rsquo;s talk about how we would actually reconstruct the continuous function from a set of samples. Let&rsquo;s re-examine our plot of the Fourier domain</p><figure><img src=reconstruction.png alt="Credit: Bracewell Fig 10.3"><figcaption><p>Credit: Bracewell Fig 10.3</p></figcaption></figure><p>The &ldquo;function&rdquo; on the left is technically not the same (continuous) function that we started with, it is a discrete representation of it. We did just say, though, that if the function was band-limited, then these samples contained all of the same information as if we had access to the full function. So how do we go from these samples back to the full function?</p><p>Let&rsquo;s look at the Fourier side of this plot and compare it to the original Fourier side. The main difference is that <em>this</em> Fourier plot has repeating &lsquo;islands&rsquo; at progressively higher frequencies, essentially to infinity. How can we get rid of these higher frequency islands?</p><p>The answer is to multiply by a boxcar function in Fourier domain, completely truncating these higher order terms. Then, we can do the inverse Fourier transform and recover the original, continuous function.</p><p>What is the analogous operation for the time-domain? This is the same thing as we discussed with the transfer function. Since it was a multiplication in the Fourier domain, it is a convolution in the time domain. And the convolutional kernel is the Fourier transform of the boxcar, which is a sinc function.</p><p>So, to <em>exactly</em> reconstruct a band-limited function from a set of samples, we do sinc-interpolation.</p><figure><a href=https://www.dsprelated.com/freebooks/pasp/Windowed_Sinc_Interpolation.html><img src=sinc-interpolation.png alt="Credit: DSP related."></a><figcaption><p>Credit: DSP related.</p></figcaption></figure><h3 id=undersampling-and-aliasing>Undersampling and aliasing</h3><p>If we didn&rsquo;t sample the function at a sufficiently high rate, then we would have overlapping islands. Essentially, the higher frequency components of the Fourier islands are &ldquo;folded-over&rdquo; back into the range of frequencies <em>we thought</em> was band-limited, resulting in a corrupted signal.</p><p>In an alias, a higher frequency signal is masquerading as a lower-frequency signal.</p><figure><img src=aliased.png alt="Credit: Bracewell Fig 10.3"><figcaption><p>Credit: Bracewell Fig 10.3</p></figcaption></figure><h3 id=compressed-sensing>Compressed sensing</h3><p>You may have heard of &ldquo;compressed sensing,&rdquo; which is one of major signal processing results of the last few decades. The idea is that you can reconstruct a functional form using far fewer samples than required for the Nyquist rate, using some dictionary of functional forms, or knowledege that the signal may be sparse. You can, indeed, perfectly reconstruct the signal through optimization using the \(L_1\) norm. If you don&rsquo;t want to make the assumption that your signal is sparse, though, it&rsquo;s a good idea to sample at the Nyquist rate.</p><h2 id=fourier-series>Fourier series</h2><p>You probably first encountered Fourier series as part of your calculus course and later on as part of a partial differential equations course.
Say we have some periodic function \(g(x)\), then the Fourier series associated with this is
$$
a_0 + \sum_1^\infty (a_n \cos 2 \pi n f x + b_n \sin 2 \pi n f x)
$$
where the Fourier coefficients are determined by
$$
a_0 = \frac{1}{T} \int_{-T/2}^{T/2} g(x) \,\mathrm{d}x
$$</p><p>$$
a_n = \frac{1}{T} \int_{-T/2}^{T/2} g(x) \cos 2 \pi n f x \,\mathrm{d}x
$$</p><p>$$
b_n = \frac{1}{T} \int_{-T/2}^{T/2} g(x) \sin 2 \pi n f x \,\mathrm{d}x
$$</p><p>i.e., we&rsquo;ve projected the function onto its basis set of sines and cosines.</p><p>Already, I&rsquo;m sure you are starting to see the close connection with what we&rsquo;ve discussed of the Fourier transform. Traditionally, Fourier series are used as a jumping off point for the discussion of the Fourier transform.</p><p>In the last lecture, however, we signaled our intention to take the opposite approach, whereby we skipped over Fourier series and <em>started</em> with the idea that Fourier transforms exist because we observe physical systems which exhibit their behavior. Now, let&rsquo;s unify the discussion and demonstrate the Fourier series as an extreme situation of the Fourier transform.</p><h3 id=fourier-transform-of-sine-and-cosine>Fourier transform of sine and cosine</h3><p>Let&rsquo;s put together a number of the theorems that we&rsquo;ve discussed to build up our understanding of what a Fourier series looks like in the Fourier domain.</p><p>In the previous lecture we introduced the Fourier transform theorem for a delta function located at the origin</p><p>$$
F(s) = \int_{-\infty}^{\infty} \delta(0) \exp (-i 2 \pi x s)\,\mathrm{d}x = 1
$$
which is a constant.</p><p>We also introduced the shift theorem, which says
$$
f(x - a) \leftrightharpoons \exp(- 2 \pi i a s) F(s).
$$</p><p>We can couple these together and write a relationship
$$
\delta(x - a) \leftrightharpoons \exp(-2 \pi i a s).
$$</p><p>Now, let&rsquo;s see if we can use this Fourier pair to derive the Fourier pairs for cosine and sine. In your quantum physics or partial differential equations classes, you probably used the Euler identity to write cosine and sine like
$$
\cos 2 \pi a s = \frac{e^{i 2 \pi a s} + e^{-i 2 \pi a s}}{2}
$$
and
$$
\sin 2 \pi a s = \frac{e^{i 2 \pi a s} - e^{-i 2 \pi a s}}{2i}
$$</p><p>So we can do a bit of rearranging and arrive at</p><p>$$
\cos \pi x \leftrightharpoons \mathrm{even}(x) = \frac{1}{2}\delta \left (x + \frac{1}{2} \right) + \frac{1}{2}\delta \left (x - \frac{1}{2} \right)
$$
and
$$
\sin \pi x \leftrightharpoons i\mathrm{odd}(x) = i \frac{1}{2}\delta \left (x + \frac{1}{2}\right) - i \frac{1}{2}\delta \left (x - \frac{1}{2}\right).
$$</p><p>where these symbols are the even and odd impulse pairs.</p><figure><img src=even-odd-impulse.png alt="The Fourier transform pairs of cosine and sine as the even and odd impulse pairs, respectively. Credit: Bracewell Fig 6.1"><figcaption><p>The Fourier transform pairs of cosine and sine as the even and odd impulse pairs, respectively. Credit: Bracewell Fig 6.1</p></figcaption></figure><p>Now we&rsquo;re well on our way to determining the Fourier spectrum of a Fourier series. The Fourier series is just a sum of sines and cosines at different frequencies. The Fourier transform is a linear operator, so we can just add together the contributions from each component in the Fourier domain
$$
a_0 + \sum_1^\infty (a_n \cos 2 \pi n f x + b_n \sin 2 \pi n f x)
$$</p><p>We arrive at the result that the spectrum of a Fourier series is a collection of delta functions whose locations and amplitudes correspond to the frequencies and values of the Fourier coefficients, respectively.</p><figure><a href="https://www.tutorialspoint.com/what-is-fourier-spectrum-theory-and-example#:~:text=The%20graph%20plotted%20between%20the,spectrum%20of%20a%20periodic%20signal.&text=Amplitude%20Spectrum%20%E2%88%92%20The%20amplitude%20spectrum,of%20Fourier%20coefficients%20versus%20frequency"><img src=line-spectra.png alt="The amplitude of the line spectra corresponding to a Fourier series. Credit: TutorialsPoint"></a><figcaption><p>The amplitude of the line spectra corresponding to a Fourier series. Credit: TutorialsPoint</p></figcaption></figure><p>Now that we&rsquo;ve derived the Fourier spectrum of a Fourier series, we can see at least two reasons why this represents an extreme situation of the Fourier transform:</p><ol><li>The input waveform is strictly periodic</li><li>The input waveform is infinite in duration</li></ol><p>As we talked about in the last lecture, these conditions are violated in the real world. If we limit the duration of the sine wave to a finite duration (say, by multiplication of a truncated Gaussian (because technically the Gaussian is also non-zero over an infinite domain), which we call a <em>window function</em>), then we see what happens in the Fourier domain: the delta functions are broadened by convolution with the Fourier transform of the window function.</p><figure><img src=broadening-line.png alt="Left: the dotted line represents a broad window function to eventually make the waveform finite in duration. Right: this has the effect of broadening the delta functions by convolution with the Fourier transform of the window function. Credit: Bracewell Fig 10.12"><figcaption><p>Left: the dotted line represents a broad window function to eventually make the waveform finite in duration. Right: this has the effect of broadening the delta functions by convolution with the Fourier transform of the window function. Credit: Bracewell Fig 10.12</p></figcaption></figure></article></main></div></div></main><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>